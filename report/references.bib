@inproceedings{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  booktitle={Advances in neural information processing systems},
  pages={3391--3401},
  year={2017}
}

@article{Geiping2020,
abstract = {The idea of federated learning is to collaboratively train a neural network on a server. Each user receives the current weights of the network and in turns sends parameter updates (gradients) based on local data. This protocol has been designed not only to train neural networks data-efficiently, but also to provide privacy benefits for users, as their input data remains on device and only parameter gradients are shared. In this paper we show that sharing parameter gradients is by no means secure: By exploiting a cosine similarity loss along with optimization methods from adversarial attacks, we are able to faithfully reconstruct images at high resolution from the knowledge of their parameter gradients, and demonstrate that such a break of privacy is possible even for trained deep networks. Moreover, we analyze the effects of architecture as well as parameters on the difficulty of reconstructing the input image, prove that any input to a fully connected layer can be reconstructed analytically independent of the remaining architecture, and show numerically that even averaging gradients over several iterations or several images does not protect the user's privacy in federated learning applications in computer vision.},
archivePrefix = {arXiv},
arxivId = {2003.14053},
author = {Geiping, Jonas and Bauermeister, Hartmut and Dr{\"{o}}ge, Hannah and Moeller, Michael},
eprint = {2003.14053},
keywords = {adversarial attacks,computer vision,deep learning,differential privacy,federated learning,inverse problems,pri-,vacy},
number = {1},
title = {{Inverting Gradients -- How easy is it to break privacy in federated learning?}},
url = {http://arxiv.org/abs/2003.14053},
year = {2020}
}

@article{Shokri2017,
abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial 'machine learning as a service' providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.},
archivePrefix = {arXiv},
arxivId = {1610.05820},
author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
doi = {10.1109/SP.2017.41},
eprint = {1610.05820},
isbn = {9781509055326},
issn = {10816011},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
pages = {3--18},
title = {{Membership Inference Attacks Against Machine Learning Models}},
year = {2017}
}

@article{Salem2019,
abstract = {Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. However, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. In addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.},
archivePrefix = {arXiv},
arxivId = {1806.01246},
author = {Salem, Ahmed and Zhang, Yang and Humbert, Mathias and Berrang, Pascal and Fritz, Mario and Backes, Michael},
doi = {10.14722/ndss.2019.23119},
eprint = {1806.01246},
isbn = {189156255X},
number = {February},
title = {{ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models}},
year = {2019}
}

@article{Fredrikson2015,
abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al. [13], adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.},
author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
doi = {10.1145/2810103.2813677},
isbn = {9781450338325},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
pages = {1322--1333},
title = {{Model inversion attacks that exploit confidence information and basic countermeasures}},
volume = {2015-Octob},
year = {2015}
}

@article{Ganju2018,
abstract = {With the growing adoption of machine learning, sharing of learned models is becoming popular. However, in addition to the prediction properties the model producer aims to share, there is also a risk that the model consumer can infer other properties of the training data the model producer did not intend to share. In this paper, we focus on the inference of global properties of the training data, such as the environment in which the data was produced, or the fraction of the data that comes from a certain class, as applied to white-box Fully Connected Neural Networks (FCNNs). Because of their complexity and inscrutability, FCNNs have a particularly high risk of leaking unexpected information about their training sets; at the same time, this complexity makes extracting this information challenging. We develop techniques that reduce this complexity by noting that FCNNs are invariant under permutation of nodes in each layer. We develop our techniques using representations that capture this invariance and simplify the information extraction task. We evaluate our techniques on several synthetic and standard benchmark datasets and show that they are very effective at inferring various data properties. We also perform two case studies to demonstrate the impact of our attack. In the first case study we show that a classifier that recognizes smiling faces also leaks information about the relative attractiveness of the individuals in its training set. In the second case study we show that a classifier that recognizes Bitcoin mining from performance counters also leaks information about whether the classifier was trained on logs from machines that were patched for the Meltdown and Spectre attacks.},
author = {Ganju, Karan and Wang, Qi and Yang, Wei and Gunter, Carl A. and Borisov, Nikita},
doi = {10.1145/3243734.3243834},
isbn = {9781450356930},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
keywords = {Neural networks,Permutation equivalence,Property inference},
pages = {619--633},
title = {{Property inference attacks on fully connected neural networks using permutation invariant representations}},
year = {2018}
}

@article{Melis2019,
abstract = {Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points - for example, specific locations - in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses.},
archivePrefix = {arXiv},
arxivId = {1805.04049},
author = {Melis, Luca and Song, Congzheng and {De Cristofaro}, Emiliano and Shmatikov, Vitaly},
doi = {10.1109/SP.2019.00029},
eprint = {1805.04049},
isbn = {9781538666609},
issn = {10816011},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
keywords = {Collaborative-learning,Deep-learning,Inference-attacks,Privacy,Security},
pages = {691--706},
title = {{Exploiting unintended feature leakage in collaborative learning}},
volume = {2019-May},
year = {2019}
}

@article{He2019,
abstract = {Deep learning has gained tremendous success and great popularity in the past few years. However, recent research found that it is suffering several inherent weaknesses, which can threaten the security and privacy of the stackholders. Deep learning's wide use further magnifies the caused consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few is clear about how these weaknesses are incurred and how effective are these attack approaches in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we are devoted to undertaking a comprehensive investigation on attacks towards deep learning, and extensively evaluating these attacks in multiple views. In particular, we focus on four types of attacks associated with security and privacy of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Many pivot metrics are devised for evaluating the attack approaches, by which we perform a quantitative and qualitative analysis. From the analysis, we have identified significant and indispensable factors in an attack vector, $\backslash$eg, how to reduce queries to target models, what distance used for measuring perturbation. We spot light on 17 findings covering these approaches' merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant researchers in this area.},
archivePrefix = {arXiv},
arxivId = {1911.12562},
author = {He, Yingzhe and Meng, Guozhu and Chen, Kai and Hu, Xingbo and He, Jinwen},
eprint = {1911.12562},
pages = {1--23},
title = {{Towards Privacy and Security of Deep Learning Systems: A Survey}},
url = {http://arxiv.org/abs/1911.12562},
year = {2019}
}

@inproceedings{shokri2015privacy,
  title={Privacy-preserving deep learning},
  author={Shokri, Reza and Shmatikov, Vitaly},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1310--1321},
  year={2015}
}

@article{DBLP:journals/corr/PapernotMGJCS16,
  author    = {Nicolas Papernot and
               Patrick D. McDaniel and
               Ian J. Goodfellow and
               Somesh Jha and
               Z. Berkay Celik and
               Ananthram Swami},
  title     = {Practical Black-Box Attacks against Deep Learning Systems using Adversarial
               Examples},
  journal   = {CoRR},
  volume    = {abs/1602.02697},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02697},
  archivePrefix = {arXiv},
  eprint    = {1602.02697},
  timestamp = {Mon, 13 Aug 2018 16:49:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PapernotMGJCS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2015faceattributes,
 title = {Deep Learning Face Attributes in the Wild},
 author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
 booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
 month = {December},
 year = {2015} 
}

@article{Li2020,
abstract = {This work studies membership inference (MI) attack against classifiers, where the attacker's goal is to determine whether a data instance was used for training the classifier. While it is known that overfitting makes classifiers susceptible to MI attacks, we showcase a simple numerical relationship between the generalization gap---the difference between training and test accuracies---and the classifier's vulnerability to MI attacks---as measured by an MI attack's accuracy gain over a random guess. We then propose to close the gap by matching the training and validation accuracies during training, by means of a new {\{}$\backslash$em set regularizer{\}} using the Maximum Mean Discrepancy between the softmax output empirical distributions of the training and validation sets. Our experimental results show that combining this approach with another simple defense (mix-up training) significantly improves state-of-the-art defense against MI attacks, with minimal impact on testing accuracy.},
archivePrefix = {arXiv},
arxivId = {2002.12062},
author = {Li, Jiacheng and Li, Ninghui and Ribeiro, Bruno},
eprint = {2002.12062},
title = {{Membership Inference Attacks and Defenses in Supervised Learning via Generalization Gap}},
url = {http://arxiv.org/abs/2002.12062},
year = {2020}
}

